<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.4">
<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-139640266-2","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="search" type="application/opensearchdescription+xml" title="Agora-cl" href="/opensearch.xml">
<script src="https://buttons.github.io/buttons.js"></script><title data-react-helmet="true">Testnet postmortems | Agora-cl</title><meta data-react-helmet="true" property="og:url" content="https://agora-cl-docs.bosagora.io/docs/reading/testnet-postmortems"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Testnet postmortems | Agora-cl"><meta data-react-helmet="true" name="description" content="Testnet Incident Reports"><meta data-react-helmet="true" property="og:description" content="Testnet Incident Reports"><link data-react-helmet="true" rel="shortcut icon" href="/img/bosagora-logo.png"><link data-react-helmet="true" rel="canonical" href="https://agora-cl-docs.bosagora.io/docs/reading/testnet-postmortems"><link data-react-helmet="true" rel="alternate" href="https://agora-cl-docs.bosagora.io/docs/reading/testnet-postmortems" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://agora-cl-docs.bosagora.io/docs/reading/testnet-postmortems" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.d023a482.css">
<link rel="preload" href="/assets/js/runtime~main.fbada0f2.js" as="script">
<link rel="preload" href="/assets/js/main.f24723cf.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#" class="skipToContent_OuoZ">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/getting-started"><img src="/img/bosagora-logo.png" alt="logo" class="themedImage_TMUO themedImage--light_4Vu1 navbar__logo"><img src="/img/bosagora-logo.png" alt="logo" class="themedImage_TMUO themedImage--dark_uzRr navbar__logo"><b class="navbar__title">Agora-cl Documentation</b></a><a href="https://github.com/zeroone-boa/agora-cl/releases/tag/v3.1.1" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">agora_v3.1.1</a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/docs/install/install-with-script">Quick Install</a><a href="https://github.com/zeroone-boa/agora-cl" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><a href="https://t.me/bosagora_eng" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>Telegram<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_wgqa"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="react-toggle toggle_2i4l react-toggle--disabled"><div class="react-toggle-track" role="button" tabindex="-1"><div class="react-toggle-track-check"><span class="toggle_iYfV">üåú</span></div><div class="react-toggle-track-x"><span class="toggle_iYfV">üåû</span></div><div class="react-toggle-thumb"></div></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div><div class="searchBox_fBfG"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_lDyR"><button class="clean-btn backToTopButton_i9tI" type="button" title="Scroll to top"><svg viewBox="0 0 24 24" width="28"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z" fill="currentColor"></path></svg></button><aside class="docSidebarContainer_0YBq"><div class="sidebar_a3j0"><nav class="menu thin-scrollbar menu_cyFh menuWithAnnouncementBar_+O1J"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/docs/getting-started">Table of contents</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/install/install-with-script">Quickstart: Run a node</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/prepare-for-merge">Prepare for The Merge</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/security-best-practices">Security best practices</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/agora-cl-usage/parameters">Command-line options</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/monitoring/checking-status">Check node and validator status</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/troubleshooting/issues-errors">Troubleshooting</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Advanced installation guides</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">How-tos</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Concepts</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#">Developer wiki</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#">Misc</a><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/audits/phase0">Security audits</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/licenses/bosagora">Agora-cl License</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/reading/eth2">Agora readings</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/docs/reading/testnet-postmortems">Testnet postmortems</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/devtools/block-explorers">Block explorers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/docs/contribute/bugreports">File a bug report</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/docs/faq">Frequently asked questions</a></li><li class="menu__list-item"><a class="menu__link" href="/docs/terminology">Glossary</a></li></ul></nav></div></aside><main class="docMainContainer_r8cw"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_zHA2"><div class="docItemContainer_oiyr"><article><div class="tocCollapsible_aw-L tocMobile_Tx6Y"><button type="button" class="clean-btn tocCollapsibleButton_zr6a">On this page</button></div><div class="markdown"><header><h1 class="h1Heading_dC7a">Testnet postmortems</h1></header><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="testnet-incident-reports"></a>Testnet Incident Reports<a class="hash-link" href="#testnet-incident-reports" title="Direct link to heading">#</a></h2><p>As part of our day to day job in building eth2 with our Agora-cl project, we are tasked with maintaining a high integrity cloud deployment that runs several nodes in our testnet. Additionally, we are always on call to determine problems which arise in the network and must be addressed by the team. This page outlines a collection of incident reports and their resolutions from catastrophic events in our testnet. We hope it will shed more light on our devops process and how we tackle the hard problems managing a distributed system such as eth2 entails.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="not-receiving-p2p-blocks-incident-7"></a>Not Receiving P2P Blocks (Incident #7)<a class="hash-link" href="#not-receiving-p2p-blocks-incident-7" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2020/04/18</p><p><strong>Authors:</strong> Preston, Raul</p><p><strong>Status:</strong> Root cause identified, resolved</p><p><strong>Network:</strong> Topaz</p><p><strong>Summary:</strong> Beacon nodes were not receiving blocks via gossipsub p2p at all, making it impossible to keep up with the chain head after initial sync completes.</p><p><strong>Reference issues:</strong>
<a href="https://github.com/zeroone-boa/agora-cl/issues/5476" target="_blank" rel="noopener noreferrer">https://github.com/zeroone-boa/agora-cl/issues/5476</a>
<a href="https://github.com/zeroone-boa/agora-cl/issues/5491" target="_blank" rel="noopener noreferrer">https://github.com/zeroone-boa/agora-cl/issues/5491</a></p><p><strong>Impact:</strong> This incident was reproducible on most node restarts, making it likely to kill our testnet if left unchecked.</p><p><strong>Root Causes:</strong> There was a race condition from the sync service needing access to the genesis time to compute a fork digest for subscribing to gossipsub topics. This value is set in the p2p service, but sometimes sync would begin before the p2p service, leading it to have a genesis time of 0.</p><p><strong>Trigger:</strong> Performing Agora node restarts could easily reproduce this issue.</p><p><strong>Resolution:</strong> We resolve the race condition by not making sync rely on p2p to set its ForkDigest, but instead get the genesis time directly from the blockchain service and refactoring ForkDigest to be a pure stateless function.</p><p><strong>Detection:</strong> Local runs, user reports.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky" title="Direct link to heading">#</a></h4><ul><li>Our pods haven‚Äôt had any rolling restarts or canaries rolling out that would trigger this issue, which is reproducible many times upon starting a Agora node. If our pods restarted, our testnet could have likely been killed and have had multiple forks.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="root-cause"></a>Root Cause<a class="hash-link" href="#root-cause" title="Direct link to heading">#</a></h4><p>Regular sync topic subscription was using a <code>forkDigest</code> of zero due to a race condition where the p2p service‚Äôs genesis time and genesis validator root was not yet initialized.</p><p>Preston realized he would see weird subscribed topics in his local node‚Äôs p2p metrics:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/00000000/attester_slashing/ssz_snappy&quot;} 0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/00000000/beacon_aggregate_and_proof/ssz_snappy&quot;} 0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/00000000/beacon_block/ssz_snappy&quot;} 0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/00000000/proposer_slashing/ssz_snappy&quot;} 0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/00000000/voluntary_exit/ssz_snappy&quot;} 0</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/f071c66c/attester_slashing/ssz_snappy&quot;} 3</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/f071c66c/beacon_aggregate_and_proof/ssz_snappy&quot;} 3</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/f071c66c/beacon_block/ssz_snappy&quot;} 2</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/f071c66c/proposer_slashing/ssz_snappy&quot;} 3</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">p2p_topic_peer_count{topic=&quot;/eth2/f071c66c/voluntary_exit/ssz_snappy&quot;} 3</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Showing duplicate values for various topics, where the 00000000 and f071c66c are the fork digests for the node. The 0 value was problematic, likely showing the sync service was started with a genesis time of 0. Preston then identified:</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline"></a>Timeline<a class="hash-link" href="#timeline" title="Direct link to heading">#</a></h4><p>2020-04-16 (all times UTC)</p><ul><li>02:00:00 Terence reports his nodes are not getting blocks via the gossip network in local chain testing, he mentions it doesn‚Äôt occur all the time and we put the issue on the backburner.</li></ul><p>2020-04-18 (all times UTC)</p><ul><li>08:02:00 Preston mentions he observes fork ENR mismatches in peer connections, Raul dismisses it as likely just a bootnode failure, and Preston also observes his local node isn‚Äôt receiving any blocks via gossip sub p2p</li><li>22:16:00 Preston tries to discover a potential race condition by making the p2p.ForkDigest() function return an error if p2p.genesisTime is not set, which ended up panicking right away, giving us a clue</li><li>22:30:00 Raul mentions there are two places where the state initialized feed fires, and they are not likely to have bugs. This feed is what notifies other services in Agora-cl of the genesis time and the genesis validators root, two critical values to compute a small hex digest used for subscribing to gossipsub topics.</li><li>22:33:00 Preston confirms the sync genesis time value is set to 0, and that we are likely subscribing to wrong gossip topics</li><li>22:35:00 Nishant mentions: ‚Äúso sync needs to wait after p2p is initialized seems like sync and p2p start at the same time which is our main problem‚Äù</li><li>22:36:00 Preston reports the ForkDigest method does not need to be a pointer receiver of the p2p service, but can instead be a pure helper function that services can fill in with canonical values from the blockchain service, preventing any potential race conditions</li><li>22:39:00 Preston reports the issue is now resolved in his local node after seeing the beacon block p2p received metrics go up p2p_message_received_total{topic=&quot;/eth2/f071c66c/beacon_block/ssz_snappy&quot;} 19</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="chain-stuck-at-genesis-incident-6"></a>Chain Stuck at Genesis (Incident #6)<a class="hash-link" href="#chain-stuck-at-genesis-incident-6" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2019/01/09</p><p><strong>Authors:</strong> Raul, Terence, Preston</p><p><strong>Status:</strong> Solved - root cause identified: <a href="https://github.com/zeroone-boa/agora-cl/issues/4526#issuecomment-573828747" target="_blank" rel="noopener noreferrer">https://github.com/zeroone-boa/agora-cl/issues/4526#issuecomment-573828747</a></p><p><strong>Summary:</strong> No one was able to propose a block after genesis time was reached with the mainnet config in our running testnet, required manual intervention.</p><p><strong>Impact:</strong> 216 skip slots after genesis. Over 30 minutes of downtime.</p><p><strong>Root Causes:</strong> Genesis block has a state root that does not match the genesis state. Still investigating other root causes.</p><p><strong>Trigger:</strong> Genesis time was reached.</p><p><strong>Resolution:</strong> Use database headBlock rather than cached head block with a fresh database. PR 4473 was deployed as a hotfix in production.</p><p><strong>Detection:</strong> DevOps observation, alerts, user reports.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky-1"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky-1" title="Direct link to heading">#</a></h4><ul><li>Minio took a backup exactly at genesis time, so we have a backup copy of the problem beacondb.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="root-cause-1"></a>Root Cause<a class="hash-link" href="#root-cause-1" title="Direct link to heading">#</a></h4><p>The problem had to do with usage of both <code>go-ssz.HashTreeRoot</code> and <code>stateutil.HashTreeRootState</code> in Agora-cl for the genesis state. Explanation outlined here: <a href="https://github.com/zeroone-boa/agora-cl/issues/4526#issuecomment-573828747" target="_blank" rel="noopener noreferrer">https://github.com/zeroone-boa/agora-cl/issues/4526#issuecomment-573828747</a></p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-1"></a>Timeline<a class="hash-link" href="#timeline-1" title="Direct link to heading">#</a></h4><p>2019-01-09 (all times UTC)</p><ul><li>8 prod nodes are online several hours before genesis time.</li></ul><p>2019-01-10</p><ul><li>00:00:00 Genesis time was reached.</li><li>00:00:00 Minio backup occurs. Link</li><li>00:00:16 Slot 1 passed, no block produced.</li><li>00:04:00 Preston observes logs that fail GetBlock RPC across all validators. &quot;Could not compute state root: could not calculate state root at slot 0: could not process block: could not process block header: parent root 0x0e6c95db700881611523e282ec3fcaeeace681786aa47928c5903650165c6165 does not match the latest block header signing root in state 0x8b25301bdeba020d4226e949eae0ecefb8863e060b7844566d51e0a42ed6490e&quot;</li><li>00:08:00 Preston restarts prod-7 and shifts all traffic there. The idea was to clear any cache value that existed before genesis start. This attempt was unsuccessful at resolving the issue.</li><li>00:15:00 All hands on deck called.</li><li>00:21:00 Preston deploys a hotfix. (See supporting info)</li><li>00:31:00 Preston shifts all traffic to prod-7, which has the hotfix. This attempt is unsuccessful.</li><li>00:33:00 Preston wipes the DB of prod-6 and restarts prod-6.</li><li>00:43:00 Preston shifts all traffic to prod-6. Blocks are starting to be produced.</li><li>00:46:00 Preston wipes all other prod databases and restarts. Prod-6 still only one serving traffic.</li><li>00:48:00 Ivan communicates to users to wipe their database and restart their nodes to sync.</li><li>00:55:00 Nodes cannot sync with --init-sync-state-cache, a nil state is attempted to be saved in the database.</li><li>00:58:00 Finality is reached.</li><li>01:10:00 Preston scales prod nodes to 10 and reverts the problem flag. This is to prevent prod-6 from getting killed if prod-7 reports healthy prematurely.</li><li>01:20:00 Preston unshifts traffic from prod-6. All healthy prod pods are serving traffic now with high gRPC success rate.</li><li>16:18:00 Preston inspects the backup database to observe that the genesis state was equal to a genesis state from running prod pod.</li><li>16:29:00 Preston inspects the backup database to observe that the genesis block state root does not match the genesis state. In the problem database, genesis block state root is 0x5a45fd74d5359fb113ec5eaa8614b652aa4ef13493b1ff9439ac7bdadaed224a and genesis state root is 0x710178cf469dcd70bb1c97630205399d26e8f0913659bb591ec9c0c1ab734f1e and the hash tree root of the problem genesis block is 0x0e6c95db700881611523e282ec3fcaeeace681786aa47928c5903650165c6165.</li><li>17:10:00 After further investigation between a healthy DB and one that was in the incident, Raul finds the genesis states in the DB match, but the genesis blocks mismatch due to state root problems. This is scary as it does not make much sense with our codebase.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="supporting-information"></a>Supporting information<a class="hash-link" href="#supporting-information" title="Direct link to heading">#</a></h4><p>The hotfix that was applied:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">diff --git a/beacon-chain/rpc/validator/proposer.go b/beacon-chain/rpc/validator/proposer.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">index 220e7efd5..46092bd73 100644</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">--- a/beacon-chain/rpc/validator/proposer.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+++ b/beacon-chain/rpc/validator/proposer.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">@@ -36,7 +36,11 @@ func (vs *Server) GetBlock(ctx context.Context, req *ethpb.BlockRequest) (*ethpb</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        // Retrieve the parent block as the current head of the canonical chain.</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-       parent := vs.HeadFetcher.HeadBlock()</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+//     parent := vs.HeadFetcher.HeadBlock()</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       parent, err := vs.BeaconDB.HeadBlock(ctx)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       if err != nil {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+               return nil, err</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       }</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        parentRoot, err := ssz.HashTreeRoot(parent.Block)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if err != nil {</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="rapid-p2p-message-drop-incident-5"></a>Rapid P2P Message Drop (Incident #5)<a class="hash-link" href="#rapid-p2p-message-drop-incident-5" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 26/11/2019</p><p><strong>Authors:</strong> Nishant</p><p><strong>Status:</strong> Root Cause Under Investigation</p><p><strong>Summary:</strong> We had a rapid drop in p2p message rates, which consequently lead to a long period without finality.</p><p><strong>Impact:</strong> Due to a sudden drop in p2p message rates, especially for attestations this lead to a long period without finality. The majority of our production pods ended up getting stuck and we started receiving large grpc failure rates.</p><p><strong>Root Causes:</strong> Yet To Be Determined</p><p><strong>Trigger:</strong> Yet to Be Determined</p><p><strong>Resolution:</strong> This was finally resolved when we scaled down our relay nodes and restarted all our out of sync production pods. The p2p message rate for attestations finally increased back to their normal levels and we got justification and finality soon after.</p><p><strong>Detection:</strong> Pinged in discord when time since last finalized epoch was greater than 10.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="lessons-learned"></a>Lessons Learned<a class="hash-link" href="#lessons-learned" title="Direct link to heading">#</a></h4><p><strong>What went wrong</strong>
There wasn‚Äôt an obvious trigger that we were able to inspect. All the major metrics and charts were fine, so it seemed from the outset that the chain was working normally as expected. However investigating the p2p message count showed another picture not captured by our current metrics and alerts.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky-2"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky-2" title="Direct link to heading">#</a></h4><ul><li>Blocks were still being produced, so that allowed us to recover the chain even though we were 5 hours out from finality.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-2"></a>Timeline<a class="hash-link" href="#timeline-2" title="Direct link to heading">#</a></h4><p>2019-01-01 (all times UTC)</p><ul><li>04:50 Nishant notices a ping on discord indicating that it has been more than 10 epochs since finality. Looking at the grafana charts, it seemed normal, so Nishant ignored it thinking it would resolve by itself.</li><li>05:00 Nishant again looks at the grafana charts but notices that there hasn‚Äôt been justification or finality for 15 mins. However all other metrics and charts are normal and there isn‚Äôt a directly obvious explanation for it.</li><li>05:05 Nishant notices that the p2p message count has dropped by a very large amount, which explains the long period without finality in the current chain. With very little attestations being sent over the wire, we have a large drop in participation rates and therefore a drop in justification and finality.</li><li>05:37 Nishant tries to rollout the current master image to all production pods.</li><li>05:50 The rollout does not bring any tangible results, with there being no justifcation or finality still. At this point it has been more than 50 epochs since finality.</li><li>07:30 Now high grpc failure rates happen across all pods. At this moment it has been more than a 100 epochs since finality. Due to that this brings a lot of stress on the agora-cl node with regards to forkchoice when determining the current head.</li><li>07:40 Nishant now reverts the connection manager fix and tries to roll the old image out to the experimental pods.</li><li>08:30 The fix however, takes a very long time to rollout due to the slow sync, which displays 2 hours to sync till the current chain head. Nishant then deploys a new fix which includes a recently merged PR for faster sync processing.</li><li>08:50 However the fix did not have the desired effect , and now grpc failure rates climbed even higher.</li><li>9:40 Preston, Terence and Raul are all now online and the situation is relayed to them.</li><li>09:45 Preston now scales down all the relay nodes to 0 to reduce the peer counts.</li><li>10:00 Preston deletes the stuck production pods, to allow only synced nodes to be hosting validators.</li><li>10:10 Attestations Message Rate increasing now.</li><li>10:12 We finally get justification</li><li>10:17 We get finality after 5 hours and the testnet survives for another day.</li><li>10:45 Now that all pods are healthy in the cluster, the relay is turned back on by Preston.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="mutex-deadlock-cascading-failures-incident-4"></a>Mutex Deadlock Cascading Failures (Incident #4)<a class="hash-link" href="#mutex-deadlock-cascading-failures-incident-4" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2019/11/23</p><p><strong>Authors:</strong> Raul Jordan, Nishant Das</p><p><strong>Status:</strong> Root cause under investigation</p><p><strong>Summary:</strong> A PR into Agora-cl (Add Lock When Accessing Checkpoints #4086)  including a mutex lock for reading and updated justified checkpoint values was checked-in to Agora-cl, pushed into a Canary deployment, and then rolled out into production after the Canary successfully passed. The feature led to deadlocks in requesting attestations and proposing blocks, creating a massive drop in validator participation eventually leading to many epochs since finality.</p><p><strong>Impact:</strong> Large number of epochs since finality created lots of context deadlines being exceeded, causing gRPC success to tank, and eventually making it hard for beacon nodes to serve any traffic as it would take too long to process a block or request attestations even after the problematic feature was reverted. This led to many cascading failures when attempting to revive the stateful set pods and rollout other old images.</p><p><strong>Root Causes:</strong> Deadlock in a very frequently used mutex. Other potential root causes still under investigation.</p><p><strong>Trigger:</strong> Canary rollout into production with commit 2f392544a6586ed7a4235a4550a2ad91dfa4a60d</p><p><strong>Resolution:</strong> Rolled back to prior image, scaled down statefulset pods that were unable to sync and left archival+experimental pods on, eventually achieved justification and finality.</p><p><strong>Detection:</strong> Ping on discord. gRPC success rate alerts at low-levels: ProposeBlock, RequestAttestation.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="lessons-learned-1"></a>Lessons Learned<a class="hash-link" href="#lessons-learned-1" title="Direct link to heading">#</a></h4><p><strong>What went well</strong>
We were able to quickly come up with hypotheses as to what went wrong, we used Jaeger to notice forkchoice.OnBlock was taking an unreasonable amount of time, and identified that it was calling the function where the mutex deadlock was occurring.</p><p><strong>What went wrong</strong>
Even after reverting the bad feature causing the original problem, it took a lot of manual effort to fix the cascading failures and other side-effects of the original problem.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky-3"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky-3" title="Direct link to heading">#</a></h4><p>We thankfully had archival and experimental pods that were working fine and as such, we were able to scale down the problematic statefulset to 0 pods and achieve justification. We got lucky our archival and experimental pods can serve traffic to validators.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-3"></a>Timeline<a class="hash-link" href="#timeline-3" title="Direct link to heading">#</a></h4><p>2019-11-23 (all times Central Standard Time)</p><ul><li>03:07 - We receive an alert on discord for &gt; 10 epochs since finality in the network</li><li>03:11 - ProposeBlock gRPC success rate drops to 83%</li><li>09:00 - Nishant raises issues about archival nodes being the potential offenders of the situation</li><li>09:31 - Nishant suggest scaling down archival nodes to 0, Preston runs the commands</li><li>09:40 - Scaling down does not work - we receive an alert in discord mentioning &gt; 25 epochs since finality</li><li>09:53 - Preston has observes very long times for spans in jaeger in proposing a block, suggesting there is some sort of mutex deadlock going on. Signatures failing on request block with the message <code>parent root does not match the latest block header signing root in state</code></li><li>10:00 - Raul begins investigating, looking in more detail at jaeger spans to increase likeliness of lock being the root cause</li><li>10:16 - Raul identifies large monitoring gap in Jaeger, with the most expensive operation in forkchoice.OnBlock not having a span. Looking at Agora-cl PR #4086 shows a function <code>updateCheckpoints</code> called in forkchoice.OnBlock without a span, providing high likelihood it is the offender</li><li>10:24 - Preston suggests rolling back all images to version before PR #4086, suggesting image tag: bosagora/agora-cl-node@sha256:7f9a060569d32a1ae05027ba2f0337b586b27dfd7a46307f552046271f1b448c. Raul proceeds and applies image to statefulset, archival deployment, and experimental deployment</li><li>10:33 - Rollback succeeded in removing the deadlock in forkchoice.OnBlock, but grpc.ProposeBlock still failing at an alarming rate. Investigation into jaeger spans concludes ProcessSlots in the state transition function is taking too long and leading to deadline exceeded problems</li><li>10:38 - Raul identifies ProcessSlots is taking an average of 5 seconds due to SSZ in the production nodes, making it nearly impossible for validators to attest or request attestations to put into blocks</li><li>10:39 - Terence suggests using the emergency flag created by Nishant called --enable-skip-slots-cache in all the beacon nodes</li><li>10:41 - Raul enables --enable-skip-slots-cache by applying a modified k8s manifest to the various deployments/statefulsets, observes &lt; 1s to propose block after being applied. Cluster still does not recover.</li><li>10:44 - Preston notices errors for AggregateAndProof in the validator client, which means we also forgot to revert the validator client images to the previous version</li><li>10:50 - No more deadline exceeded errors, but chain is not recovering. There are still problems with many nodes being stuck after initial sync and seemingly being restarted.</li><li>11:34 - ProposeBlock and RequestAttestation still failing, still many epochs since finality, Raul notices pods keep getting restarted and rescheduled, but apparently archival and experimental nodes are still functional and advancing our chain</li><li>12:16 - Confirmed via Kibana pods finish initial sync in the statefulset but at the end have unhealthy status <code>Node is unhealthy! ERROR syncing\n*sync.RegularSync: ERROR waiting for initial sync</code></li><li>12:18 - Raul tracks logs for all beacon-chain-prod nodes and notices they always are unhealthy after initial sync but there are no error logs, issues persist</li><li>12:30 - Nishant resumes investigation, decides to scale down statefulset prod nodes to 0</li><li>12:52 - Network achieves justification</li><li>12:56 - Network achieves finality</li><li>13:00 - Network is all operational, systems back to normal, gRPC back to 100%. However, only running with archival + experimental pods. Statefulset is still at 0 pods.</li><li>15:30 - Statefulset scaled back to 5</li></ul><p>11/24/2019</p><ul><li>18:00 - Alert received for no finality in 10 Epochs.</li><li>19:20 - Terence confirm with Danny that the finality reversions we have been witnessing is indeed a spec bug. However, there isn‚Äôt an easy fix that we can deploy immediately, so we will have to wait till the next release.</li><li>20:30 - Nishant points to a Pull Request that modified the logic behind our pending queues that could have been the cause of our finality issues.He then opens up a PR to revert it.</li><li>20:50 Preston rolls out the hotfix with the PR reverted to our experimental nodes.</li><li>21:30 Seeing as the rollout did not have any negative effect on our experimental nodes, Preston then pushes the hotfix to our production nodes.</li><li>22:30 The hotfix has been successfully rolled out</li><li>23:30 We still get alerts with no finality for 10 epochs. Which means that the hotfix was unsuccessful.</li></ul><p>11/25/2019</p><ul><li>0:30 After looking at all our previous PRs and observing network metrics for the past few days, Preston suggests that the main cause of our finality worries isn‚Äôt a bad PR merged in , but instead the growth of the network. With peer counts increasing close to 70, each Agora node processes close to 200 attestations /sec</li><li>1:00 Preston then checks Jaeger and see that a very large amount of time is spent on DB writes especially in updateAttVotes in OnAttestation. The span in Jaeger, supports this theory as it shows OnAttestation taking nearly 1s with the large majority being taken up by writing attestation data to disk. Initially it looked like an improper use of Batch Updates in Bolt.</li><li>1:10 After exploring the code in some more, Nishant finds the problem is not that but instead the fact that the function upateAttVotes is very heavy on disk I/O due to the latest vote being saved for each validator in the attestation. This coupled with the fact that the Agora node receives 200 attestations/sec , it would explain the large amounts of time being spent on db writes in Jaeger.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="supporting-information-1"></a>Supporting information<a class="hash-link" href="#supporting-information-1" title="Direct link to heading">#</a></h4><p>Statefulset prod pods being stuck even after a rollout of a previous image was completed - pods showed no error logs but after finishing regular sync, they were unhealthy and were unable to serve traffic to validators. Archival+experimental pods were properly carrying out the chain on their own.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="finality-decreased-incident-3"></a>Finality Decreased! (Incident #3)<a class="hash-link" href="#finality-decreased-incident-3" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2019/11/21</p><p><strong>Authors:</strong> Preston, Terence</p><p><strong>Status:</strong> Mitigated; In root cause investigation</p><p><strong>Summary:</strong> Agora-cl nodes observed a finalized checkpoint reversion which caused all attestations to be rejected.</p><p><strong>Impact:</strong> 56 epochs gap in finality</p><p><strong>Root Causes:</strong> Agora-cl clients experienced a spec allowed finality reversion during a forking condition. Agora-cl‚Äôs chain service maintains the most recent finalized checkpoint and has validation to ensure a finalized checkpoint can never revert. This led to a difference in finalized checkpoints between the chain service cache and the head state. All incoming attestations were validated and rejected based on the chain service checkpoint.</p><p><strong>Trigger:</strong> Finality decreases</p><p><strong>Resolution:</strong> TBD -- Fix the condition for updating finalized check point cache</p><p><strong>Detection:</strong> Prometheus, pager</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="action-items"></a>Action Items<a class="hash-link" href="#action-items" title="Direct link to heading">#</a></h4><ul><li>Confirm finality roll back is normal</li></ul><p>Confirmed it‚Äôs not normal. From Danny: The fork choice first narrows down the block tree by finality, then narrows it down by latest justified. In the above scenario, the block at slot x+8 should not even be considered in the fork choice</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="lessons-learned-2"></a>Lessons Learned<a class="hash-link" href="#lessons-learned-2" title="Direct link to heading">#</a></h4><p><strong>What went well</strong>
The ability to extract offender DB and play back these scenarios is very helpful
Prometheus graphs helped to narrow down the problem</p><p><strong>What went wrong</strong>
Lack of metrics. For a single entry, we often have to consider 2 values. The latest value, and the cached value. In this case, we only captured the latest value</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-4"></a>Timeline<a class="hash-link" href="#timeline-4" title="Direct link to heading">#</a></h4><p>2019-11-21 (all times UTC)</p><ul><li>23:35:26 - Received  block 74137 with pre state of slot 74132. The pre state has finalized epoch is 9264.</li><li>23:35:26~23:36:50 - No blocks received.</li><li>23:36:50 - Received block 74144 with pre state of slot 74130 (in epoch 9266). The pre state has finalized epoch 9263. Attestations favored block from 74144 and made post state of slot 74144 the head state which has finalized epoch 9263. Database headState was updated where the finalized epoch is now 9263. ChainService.HeadState (cache) is also updated with the new state with finalized epoch of 9263. The chain service FinalizedCheckpt was not updated due to an enforcement that the finalized checkpoint cannot drop.</li><li>23:37 - Prometheus observed finalized epoch drop from 9264 to 9263. Link. This affected all Prylabs nodes at the same time.</li><li>23:37 - P2P attestations in Agora chain nodes that attestations were ‚Äúold‚Äù since their source was 6264. Nodes were producing attestations with the finalized checkpoint epoch at 9263, but nodes were validating them in p2p against 9264.</li><li>23:47 - Page is received. Preston begins investigation.</li></ul><p>2019-11-22</p><ul><li>00:00 - Preston suspects something is going on with the finalized checkpoint.</li><li>~00:10 - Normal canary rollout proceeds. Node restarts are welcome to flush any cache.</li><li>~00:25 - Rollout complete, problem persists.</li><li>00:40 - Preston suspects the finalized checkpoint in the chain info reverted but the checkpoint in the head state did not. This line of code seemed to be the cause of our attestations being rejected.</li><li>00:44 - Hotfix patch rollout begins. (See supporting info for patch details).</li><li>00:55 - Rollout complete, 1 pod struggling. Experimental, archival deployments scaled down.</li><li>00:58 - Justification</li><li>01:00 - Finalization</li><li>01:00 - Hotfix patch is rolled back. Experimental, archival deployments scaled up.</li><li>01:04 - Problem mitigation ends and root cause investigation begins.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="supporting-information-2"></a>Supporting information<a class="hash-link" href="#supporting-information-2" title="Direct link to heading">#</a></h4><p>Supporting test: <a href="https://github.com/zeroone-boa/agora-cl/blob/454c7d70e69e98e39d26f862c306b352a5381f08/beacon-chain/core/state/transition_test.go#L1109" target="_blank" rel="noopener noreferrer">https://github.com/zeroone-boa/agora-cl/blob/454c7d70e69e98e39d26f862c306b352a5381f08/beacon-chain/core/state/transition_test.go#L1109</a></p><p>Additional supporting data: <a href="https://gist.github.com/prestonvanloon/365cc64804b46bd65790ba898cf60e1b" target="_blank" rel="noopener noreferrer">https://gist.github.com/prestonvanloon/365cc64804b46bd65790ba898cf60e1b</a></p><p>Hotfix patch that was applied:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI"><pre tabindex="0" class="prism-code language-undefined codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token plain">diff --git a/beacon-chain/sync/validate_beacon_attestation.go b/beacon-chain/sync/validate_beacon_attestation.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">index 30671f9b9..9684609c8 100644</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">--- a/beacon-chain/sync/validate_beacon_attestation.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+++ b/beacon-chain/sync/validate_beacon_attestation.go</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">@@ -65,7 +65,11 @@ func (r *RegularSync) validateBeaconAttestation(ctx context.Context, msg proto.M</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                return false, nil</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">-       finalizedEpoch := r.chain.FinalizedCheckpt().Epoch</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       head, err := r.db.HeadState(ctx)</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       if err != nil {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+               return false, err</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       }</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+       finalizedEpoch := head.FinalizedCheckpoint.Epoch</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        attestationDataEpochOld := finalizedEpoch &gt;= att.Data.Source.Epoch || finalizedEpoch &gt;= att.Data.Target.Epoch</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if finalizedEpoch != 0 &amp;&amp; attestationDataEpochOld {</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                log.WithFields(logrus.Fields{</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(END)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Conversation with Danny:
Danny Ryan, [Nov 24, 2019 at 11:16:04 AM]:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Check block is a descendant of the finalized block</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">assert</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        get_ancestor</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> signing_root</span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain">block</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">blocks</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">root</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">slot</span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> </span><span class="token operator">==</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">root</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>should prevent adding new blocks that are not descendants of the finalized block.</p><p>There is a corner case in which a branch might have the finalized block but not enough votes on the branch to have <em>finalized</em> the block. Such a case does not currently have a fix in the spec, but the fix you added doesn‚Äôt properly cover this case because it cuts out possible branches that <em>might</em> finalize the root in the future (include proper amount of attestations).</p><p>We‚Äôve been talking about this scenario the past week. Not 100% sure the scenario you found is the same</p><p>regardless, the following should prevent you from finalizing an older epoch in any possible case</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI python"><pre tabindex="0" class="prism-code language-python codeBlock_rtdJ thin-scrollbar" style="color:#F8F8F2;background-color:#282A36"><code class="codeBlockLines_1zSZ"><span class="token-line" style="color:#F8F8F2"><span class="token comment" style="color:rgb(98, 114, 164)"># Update finalized checkpoint</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">if</span><span class="token plain"> state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epoch </span><span class="token operator">&gt;</span><span class="token plain"> store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">epoch</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain"></span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        store</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint </span><span class="token operator">=</span><span class="token plain"> state</span><span class="token punctuation" style="color:rgb(248, 248, 242)">.</span><span class="token plain">finalized_checkpoint</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">you _never_ finalize an older epoch </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> on_block</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">and</span><span class="token plain"> get_head filters out blocks that aren‚Äôt </span><span class="token keyword" style="color:rgb(189, 147, 249);font-style:italic">in</span><span class="token plain"> your the block tree rooted at your latest justified</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB clean-btn">Copy</button></div></div><p>Terence Tsao, [Nov 24, 2019 at 11:35:13 AM]:
But wouldn‚Äôt you want to prevent adding new block that‚Äôs not descendent of the ‚Äúlatest‚Äù finalized block? Here‚Äôs what happened in our node:</p><p>After processing b144, store.finalized_epoch is 4 because it cant be reverted given the &quot;update finalized checkpoint...&quot; condition. But head_state.finalized_epoch got reverted to 3</p><p>The &quot;descendant of the finalized block...&quot; check doesn&#x27;t cover this case because block111 (finalized epoch 4) is also a descendent of b101 (finalized epoch 3)</p><p>Only thing I think of is get_head did not filter out b144 based on &quot;block tree rooted at your latest justified...&quot; condition‚Ä¶ I‚Äôll check that now</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="goerli--pow-nodes-offline-killed-testnet-incident-2"></a>Goerli / PoW Nodes Offline Killed Testnet (Incident #2)<a class="hash-link" href="#goerli--pow-nodes-offline-killed-testnet-incident-2" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2019/10/30</p><p><strong>Authors:</strong> Preston and Nishant</p><p><strong>Status:</strong> Action items in progress</p><p><strong>Summary:</strong> Goerli PoW test network experienced a hard fork which the Agora-cl testnet nodes were not compatible with. As a result, all connections to Goerli PoW chain from Agora-cl nodes failed and the condition could not be recovered before too many epochs had lapsed to the point that Agora-cl can not compute assignments fast enough.</p><p><strong>Impact:</strong> Agora-cl testnet stalled for 2 hours. Committee assignments could not be calculated within adequate time without substantial code changes. Agora-cl testnet required a restart.</p><p><strong>Root Causes:</strong> Tight coupling with PoW. Committee assignments fail to compute within a short deadline.</p><p><strong>Trigger:</strong> Istanbul hardfork on Goerli.</p><p><strong>Resolution:</strong> Full testnet restart, scheduled for November 4th.</p><p><strong>Detection:</strong> Ping on twitter / gitter / discord. Chain stalled alert.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="lessons-learned-3"></a>Lessons Learned<a class="hash-link" href="#lessons-learned-3" title="Direct link to heading">#</a></h4><p><strong>What went well</strong></p><ul><li>go-ethereum was able to sync Goerli in under 15 minutes.</li></ul><p><strong>What went wrong</strong></p><ul><li>Critical monitoring alerts didn‚Äôt fire with missing data.</li><li>Lack of communication between Prysmatic Labs and Goerli community caused an easily preventable issue.</li><li>Inability to start without ETH1 caused the chain to be 100% offline.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky-4"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky-4" title="Direct link to heading">#</a></h4><p>This incident exposes a single point of failure with PoW: Agora-cl nodes could not start without an existing PoW connection, despite being recently synced.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-5"></a>Timeline<a class="hash-link" href="#timeline-5" title="Direct link to heading">#</a></h4><p>2019-10-30 (all times UTC)</p><ul><li>13:36 soc1c announces hardfork happens within one hour.</li><li>13:53 Istanbul hardfork happens at block 1561651. All of Agora-cl goerli nodes were not compatible and could not advance the chain.</li><li>~13:55 Beacon chain nodes stop serving traffic. All are reporting unhealthy due to lack of  healthyETH1 connection.</li><li>14:05 Chain starts to stall. See chart.</li><li>~14:10 Beacon chain nodes have been unhealthy for too long. Kubernetes scheduler starts to reschedule the pods, but pods cannot start.</li><li>14:13 Chain stall metric drops off with no data. Presumably because all Agora chain nodes are dead and the metric is not being scraped/reported.</li><li>15:11 Peter reaches out on twitter.</li><li>15:11 Chain stalled alert fired.</li><li>15:16 Chain stalled alert resolved. (Due to lack of data).</li><li>16:00 Prysmatic Goerli nodes updated, syncing from genesis.</li><li>16:10 Prysmatic Goerli nodes back online.</li><li>~16:12 Hacked image deployed to be immediately in sync.</li><li>16:14 Agora-cl nodes can start again and come online.</li><li>~16:14 A beacon block is produced, stalled chain event fires again.</li><li>16:22 All Agora-cl nodes back online, except prod-4 and prod-1(expected those are stuck due to issue 3885).</li><li>~16:22 Hacked image rolled back.</li><li>16:31 Nishant observes deadline exceeded at high rate for committee assignments. The chain is considered unrecoverable.</li><li>16:31 Investigation ends.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="testnet-large-finality-gap-postmortem-incident-1"></a>Testnet Large Finality Gap Postmortem (Incident #1)<a class="hash-link" href="#testnet-large-finality-gap-postmortem-incident-1" title="Direct link to heading">#</a></h3><p><strong>Date:</strong> 2019-10-27</p><p><strong>Authors:</strong> Preston Van Loon</p><p><strong>Status:</strong> Draft</p><p><strong>Summary:</strong> A seemingly benign production rollout uncovered a cascading series of errors which led to a large gap in finalized epochs.</p><p><strong>Impact:</strong> 86 consecutive epochs without finality.</p><p><strong>Root Causes:</strong> Offline validators, forked Agora chain pods.</p><p><strong>Trigger:</strong> A successful 3 hour canary report started a rollout to production and experimental pods.</p><p><strong>Resolution:</strong> Validators were penalized and ejected until finality could be reached.</p><p><strong>Detection:</strong> Prometheus alarm for too_long_since_finality_10.</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="lessons-learned-4"></a>Lessons Learned<a class="hash-link" href="#lessons-learned-4" title="Direct link to heading">#</a></h4><p><strong>What went well</strong>
Finality alert notified on call and response time was less than a minute.</p><p><strong>What went wrong</strong></p><ul><li>There are multiple issues causing finality gap and it was hard to isolate the problem.</li><li>We allowed too many external peers to join the testnet and go offline. At the time of the event, we only controlled 70% of active validators which is dangerously close to falling under 2/3rds majority. Given the ephemeral nature of external testers, we can assume that the majority of validators not controlled by Prysmatic Labs are offline. Any issue in the network is extra sensitive to finality gaps.</li><li>There is an existing bug where non-canonical blocks are served on p2p RPC requests for blocks by range. This is blocking testnet users from syncing to head and is blocking normal rollout procedures for beacon-chain-prod-1.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="where-we-got-lucky-5"></a>Where we got lucky<a class="hash-link" href="#where-we-got-lucky-5" title="Direct link to heading">#</a></h4><ul><li>Some validators were close to ejection. After enough of the exponential penalties, the network was able to reach finality. Had this event continued beyond 100 epochs, it is very likely that we would not have been able to recover due to timeouts in committeeAssignment requests as it grows more expensive as the gap in finality increases.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="timeline-6"></a>Timeline<a class="hash-link" href="#timeline-6" title="Direct link to heading">#</a></h4><p>2019-10-27 (all times UTC)</p><ul><li>3:47:11 Canary pipeline starts.</li><li>7:09:34 Prod pods deployment starts.</li><li>7:09:34 Experimental pods deployment starts.</li><li>7:13:00 beacon-chain-experimental-6dcdd678d4-2j2vp, beacon-chain-experimental-6dcdd678d4-s62lv, and beacon-chain-canary-5b4c95f7f6-nf8st starts logging ‚Äúcould not verify indexed attestation: attestation aggregation signature did not verify‚Äù at 5 to 15 times per minute until 7:40:00.</li><li>7:14:24 First sign of issues in finality. 6.785 epochs since last finality. Finality is reached.</li><li>7:14:52 Finalized epoch reaches 17332, then drops to 17331.</li><li>7:16:16 beacon-chain-experimental-6dcdd678d4-s62lv experiences less than 95% gRPC success rate for ProposeBlock. This pod is running the new image from rollout.</li><li>7:16:44 Finalized epoch reaches 17332, then drops to 17331.</li><li>7:19:32 beacon-chain-prod-0 experiences less than 95% gRPC success rate for ProposeBlock. This pod is running the old image and has not been restarted in over two days due to a stuck rollout with prod-1.</li><li>7:19:32 beacon-chain-prod-2 experiences less than 95% gRPC success rate for ProposeBlock. Uncertain if this pod was recently restarted or running old image.</li><li>7:19:32 Several pods experience high rate of p2p processing failures on beacon block gossip topic.</li><li>7:20:00 too_long_since_finality_10 alert starts.</li><li>7:20:00 Preston ACKs alert and begins investigation.</li><li>7:27:00 Preston observes block proposals are failing on three pods.</li><li>7:27:00 Preston observes finalized_epoch metric drop by 1.</li><li>7:33:04 Finality is reached again. This gap was 26 epochs.</li><li>7:37:00 Preston observes many ‚Äúcould not verify indexed attestation: attestation aggregation signature did not verify‚Äù errors from receiving attestations of p2p.</li><li>7:40:00 Preston scales experimental pods to 0.</li><li>7:45:00 Preston pauses canary pipeline and cancels currently running canary. Canary pod is deleted.</li><li>8:38:52 Finality is reached. This gap was 86 epochs.</li><li>8:44:00 Investigation ends.</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><a href="https://github.com/zeroone-boa/agora-cl-docs/edit/master/website/docs/reading/testnet_postmortems.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_mS5F" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_wj+Z"></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/reading/eth2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">¬´ Agora readings</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/devtools/block-explorers"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Block explorers ¬ª</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_vrFS thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#testnet-incident-reports" class="table-of-contents__link">Testnet Incident Reports</a><ul><li><a href="#not-receiving-p2p-blocks-incident-7" class="table-of-contents__link">Not Receiving P2P Blocks (Incident #7)</a></li><li><a href="#chain-stuck-at-genesis-incident-6" class="table-of-contents__link">Chain Stuck at Genesis (Incident #6)</a></li><li><a href="#rapid-p2p-message-drop-incident-5" class="table-of-contents__link">Rapid P2P Message Drop (Incident #5)</a></li><li><a href="#mutex-deadlock-cascading-failures-incident-4" class="table-of-contents__link">Mutex Deadlock Cascading Failures (Incident #4)</a></li><li><a href="#finality-decreased-incident-3" class="table-of-contents__link">Finality Decreased! (Incident #3)</a></li><li><a href="#goerli--pow-nodes-offline-killed-testnet-incident-2" class="table-of-contents__link">Goerli / PoW Nodes Offline Killed Testnet (Incident #2)</a></li><li><a href="#testnet-large-finality-gap-postmortem-incident-1" class="table-of-contents__link">Testnet Large Finality Gap Postmortem (Incident #1)</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer"><div class="container"><div class="footer__bottom text--center"><div class="margin-bottom--sm"><a class="footerLogoLink_SRtH" href="/docs/getting-started"><img src="/img/Agora-cl.svg" alt="Agora-cl Docs" class="themedImage_TMUO themedImage--light_4Vu1 footer__logo"><img src="/img/Agora-cl.svg" alt="Agora-cl Docs" class="themedImage_TMUO themedImage--dark_uzRr footer__logo"></a></div><div class="footer__copyright">Copyright ¬© 2022 Bosagora, Validator Deposit Contract 0xXXX</div></div></div></footer></div>
<script src="/assets/js/runtime~main.fbada0f2.js"></script>
<script src="/assets/js/main.f24723cf.js"></script>
</body>
</html>